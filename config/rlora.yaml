initialize_from_hf: "Qwen/Qwen2.5-Coder-1.5B"
full_ft: false
embedding_router_token_ft: false
data:
  train_urls:
    - gs://usc-stack-v2/shard1_150_flat.jsonl.gz
  validation_urls:
    - gs://usc-stack-v2/shard1_215_220_flat.jsonl.gz
  cache_dir: "gs://usc2-datacache/shard1"
  tokenizer: "Qwen/Qwen2.5-Coder-1.5B"
  shuffle: true
  predict_prefix: false
  predict_router_token: false
  predict_fim_token: false
  add_router_token: false
model:
  seq_len: 8192
  hidden_dim: 1536
  intermediate_dim: 8960
  num_heads: 12
  num_layers: 28
  num_kv_heads: 2
  layer_norm_epsilon: 1e-6
  rope:
    type: default
    theta: 1000000
  tie_word_embeddings: true

  use_flash_attention: true
  attn_backend: jax_flash
  flash_attention_block_size: 512
  use_bias: false
  use_layer_norm_weight: true

  disable_lora_mask: false
  ident_lora_mask: false

  num_loras: 64
  lora_rank: 8
  top_k: 8
  scale: 1.0

trainer:
  tracker:
    type: wandb
    project: "levanter-rlora"
    tags: ["high_batch_rlora"]
  mp: p=f32,c=bfloat16
  train_batch_size: 256
  per_device_parallelism: 2
  num_train_steps: 200
  steps_per_eval: 10
  max_eval_batches: 64
  per_device_eval_parallelism: 4
  tensor_parallel_axes: ["mlp", "heads"]
  fsdp_axis: "embed"
  batch_axis: "batch"
  checkpointer:
    base_path: gs://usc2-ckpt
    save_interval: 15m

optimizer:
  type: adam
  learning_rate: 6e-3
  weight_decay: 0.0
  warmup: 0.01
